{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":9000444,"datasetId":5418758,"databundleVersionId":9166653},{"sourceType":"datasetVersion","sourceId":9010871,"datasetId":5400149,"databundleVersionId":9177807},{"sourceType":"datasetVersion","sourceId":9012610,"datasetId":5424550,"databundleVersionId":9179656},{"sourceType":"datasetVersion","sourceId":2739456,"datasetId":1670098,"databundleVersionId":2784614}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def add_to_class(Class):\n    def wrapper(obj):\n        setattr(Class, obj.__name__, obj)\n    return wrapper","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:29:03.212626Z","iopub.execute_input":"2024-07-23T12:29:03.213757Z","iopub.status.idle":"2024-07-23T12:29:03.246216Z","shell.execute_reply.started":"2024-07-23T12:29:03.213716Z","shell.execute_reply":"2024-07-23T12:29:03.245003Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from typing import Optional, Callable, List, Tuple\n\nimport torch\nfrom torch import nn\nimport torchaudio\nimport librosa\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport math\n\nimport re\nimport os\n\nimport IPython.display as ipd","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:29:03.248107Z","iopub.execute_input":"2024-07-23T12:29:03.248434Z","iopub.status.idle":"2024-07-23T12:29:08.140323Z","shell.execute_reply.started":"2024-07-23T12:29:03.248407Z","shell.execute_reply":"2024-07-23T12:29:08.139178Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/asr-helper')\n\nfrom Conformer import ConformerEncoderLayer, ConformerEncoder\nfrom RNNT import _TimeReduction, _Predictor_Kiss, _Predictor, _Joiner, RNNT\nfrom Tokenizer import BPETokenizer\nimport S4T as S","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:29:08.141801Z","iopub.execute_input":"2024-07-23T12:29:08.142272Z","iopub.status.idle":"2024-07-23T12:29:08.232482Z","shell.execute_reply.started":"2024-07-23T12:29:08.142244Z","shell.execute_reply":"2024-07-23T12:29:08.231187Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:29:08.234087Z","iopub.execute_input":"2024-07-23T12:29:08.234441Z","iopub.status.idle":"2024-07-23T12:29:08.242860Z","shell.execute_reply.started":"2024-07-23T12:29:08.234412Z","shell.execute_reply":"2024-07-23T12:29:08.241438Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'cpu'"},"metadata":{}}]},{"cell_type":"markdown","source":"## LibriSpeech100","metadata":{}},{"cell_type":"code","source":"TRANSFORM = nn.Sequential(torchaudio.transforms.MelSpectrogram(sample_rate = 16000,\n                                                                 n_fft = 512,\n                                                                 win_length = 400,\n                                                                 hop_length = 160,\n                                                                 n_mels = 80),\n                          torchaudio.transforms.AmplitudeToDB())\nTRAIN_TRANSFORM = torchaudio.transforms.SpecAugment(n_time_masks = 10,\n                                      time_mask_param = 10,\n                                      n_freq_masks = 1,\n                                      freq_mask_param = 27)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:29:08.245767Z","iopub.execute_input":"2024-07-23T12:29:08.246187Z","iopub.status.idle":"2024-07-23T12:29:08.363944Z","shell.execute_reply.started":"2024-07-23T12:29:08.246113Z","shell.execute_reply":"2024-07-23T12:29:08.362540Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#\nPAD_IDX = 0\nUNK_IDX = 1\nBOS_IDX = 2\nEOS_IDX = 3","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:29:08.366000Z","iopub.execute_input":"2024-07-23T12:29:08.366377Z","iopub.status.idle":"2024-07-23T12:29:08.372095Z","shell.execute_reply.started":"2024-07-23T12:29:08.366346Z","shell.execute_reply":"2024-07-23T12:29:08.370649Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class LibriSpeech100(torch.utils.data.Dataset):\n    def __init__(self, root, subset = 'train'):\n        super().__init__()\n        self.subset = subset\n        self.dataset = torchaudio.datasets.LIBRISPEECH(root, url = subset)\n\n    def __getitem__(self, idx):\n        wav, _, text, *_ = self.dataset[idx]\n        wav = TRANSFORM(wav)\n        if 'train' in self.subset:\n            wav = TRAIN_TRANSFORM(wav)\n        return wav, text\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:29:08.374197Z","iopub.execute_input":"2024-07-23T12:29:08.375333Z","iopub.status.idle":"2024-07-23T12:29:08.384265Z","shell.execute_reply.started":"2024-07-23T12:29:08.375299Z","shell.execute_reply":"2024-07-23T12:29:08.383083Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class LS100(S.SDataModule):\n    def __init__(self, root, batch_size):\n        super().__init__()\n        self.root = root\n        self.batch_size = batch_size\n        train100 = LibriSpeech100(root,\n                                 subset = 'train-clean-100')\n        train360 = LibriSpeech100(root,\n                                 subset = 'train-clean-360')\n        self.train_dataset = torch.utils.data.ConcatDataset([train100, train360])\n        #self.train_dataset = train360\n        self.val_dataset = LibriSpeech100(root,\n                               subset = 'dev-clean')\n        self.test_dataset = LibriSpeech100(root,\n                                subset = 'test-clean')\n        self.tokenizer = BPETokenizer('/kaggle/input/asr-helper/BPEvocab120.json',\n                                     '/kaggle/input/asr-helper/BPEsplits120.json')\n\n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(self.train_dataset,\n                                           batch_size = self.batch_size,\n                                           shuffle = True,\n                                           collate_fn = self.collate_fn,\n                                           num_workers = 4,\n                                           prefetch_factor = 1,\n                                           pin_memory = True,\n                                           drop_last = False)\n\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(self.val_dataset,\n                                           batch_size = self.batch_size,\n                                           shuffle = False,\n                                           collate_fn = self.collate_fn,\n                                           num_workers = 4,\n                                           prefetch_factor = 1,\n                                           pin_memory = True)\n\n    def test_dataloader(self):\n        return torch.utils.data.DataLoader(self.test_dataset,\n                                           batch_size = 1,\n                                           shuffle = False,\n                                           collate_fn = self.collate_fn,\n                                           num_workers = 1,\n                                           prefetch_factor = 1,\n                                           pin_memory = True)\n\n    def collate_fn(self, batch):\n        src_batch, src_lengths, tgt_batch, tgt_lengths = [], [], [], []\n        for src_sample, tgt_sample in batch:\n            tgt_sample = torch.tensor(self.tokenizer(tgt_sample))\n            src_batch.append(src_sample.squeeze(0).transpose(0, 1).contiguous())\n            tgt_batch.append(tgt_sample)\n            src_lengths.append(src_sample.shape[2])\n            tgt_lengths.append(len(tgt_sample)-1)\n\n        src_batch = nn.utils.rnn.pad_sequence(src_batch, batch_first = True, padding_value = 0)\n        tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, batch_first = True, padding_value = PAD_IDX)\n        src_lengths = torch.tensor(src_lengths)\n        tgt_lengths = torch.tensor(tgt_lengths)\n        return src_batch, tgt_batch.type(torch.int32), src_lengths.type(torch.int32), tgt_lengths.type(torch.int32)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:29:08.386380Z","iopub.execute_input":"2024-07-23T12:29:08.386919Z","iopub.status.idle":"2024-07-23T12:29:08.405146Z","shell.execute_reply.started":"2024-07-23T12:29:08.386880Z","shell.execute_reply":"2024-07-23T12:29:08.403540Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data = LS100('/kaggle/input/librispeech-clean', 8)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:29:08.407114Z","iopub.execute_input":"2024-07-23T12:29:08.407762Z","iopub.status.idle":"2024-07-23T12:30:38.784458Z","shell.execute_reply.started":"2024-07-23T12:29:08.407717Z","shell.execute_reply":"2024-07-23T12:30:38.783212Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class _ConformerEncoder(nn.Module):\n    def __init__(self,\n                 input_dim: int,\n                 output_dim: int,\n                 time_reduction_stride: int,\n                 conformer_input_dim: int,\n                 conformer_ffn_dim: int,\n                 conformer_num_layers: int,\n                 conformer_num_heads: int,\n                 conformer_depthwise_conv_kernel_size: int,\n                 conformer_dropout: float) -> None:\n        super().__init__()\n        self.time_reduction = _TimeReduction(time_reduction_stride)\n        self.input_linear = nn.Linear(input_dim*time_reduction_stride, conformer_input_dim)\n        conformerencoderlayer = ConformerEncoderLayer(input_dim = conformer_input_dim,\n                                                      ffn_dim = conformer_ffn_dim,\n                                                      num_heads = conformer_num_heads,\n                                                      kernel_size = conformer_depthwise_conv_kernel_size,\n                                                      dropout = conformer_dropout)\n        self.conformer = ConformerEncoder(conformerencoderlayer,\n                                          num_layers = conformer_num_layers)\n        self.output_linear = nn.Linear(conformer_input_dim, output_dim)\n        self.layer_norm = nn.LayerNorm(output_dim)\n\n    def forward(self,\n                input: torch.Tensor,\n                lengths: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        input: (N, T, D)\n        length: (N,)\n        \"\"\"\n        time_reduction_out, time_reduction_lengths = self.time_reduction(input, lengths)\n        input_linear_out = self.input_linear(time_reduction_out)\n        x, lengths, _ = self.conformer(input_linear_out, time_reduction_lengths)\n        output_linear_out = self.output_linear(x)\n        layer_norm_out = self.layer_norm(output_linear_out)\n        return layer_norm_out, lengths","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:30:38.785747Z","iopub.execute_input":"2024-07-23T12:30:38.786062Z","iopub.status.idle":"2024-07-23T12:30:38.796679Z","shell.execute_reply.started":"2024-07-23T12:30:38.786036Z","shell.execute_reply":"2024-07-23T12:30:38.795244Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class ConformerTransducer(RNNT):\n    def __init__(self,\n                 input_dim: int,\n                 output_dim: int,\n                 time_reduction_stride: int,\n                 conformer_input_dim: int,\n                 conformer_ffn_dim: int,\n                 conformer_num_layers: int,\n                 conformer_num_heads: int,\n                 conformer_depthwise_conv_kernel_size: int,\n                 conformer_dropout: float,\n                 num_symbols: int,\n                 symbol_embedding_dim: int,\n                 num_lstm_layers: int,\n                 lstm_hidden_dim: int,\n                 lstm_layer_norm: bool = False,\n                 lstm_layer_norm_epsilon: float = 1e-5,\n                 lstm_dropout: float = 0.0,\n                 joiner_activation: str = 'relu') -> None:\n        transcriber = _ConformerEncoder(input_dim,\n                                             output_dim,\n                                             time_reduction_stride,\n                                             conformer_input_dim,\n                                             conformer_ffn_dim,\n                                             conformer_num_layers,\n                                             conformer_num_heads,\n                                             conformer_depthwise_conv_kernel_size,\n                                             conformer_dropout)\n        predictor = _Predictor(num_symbols,\n                                    output_dim,\n                                    symbol_embedding_dim,\n                                    num_lstm_layers,\n                                    lstm_hidden_dim,\n                                    lstm_layer_norm,\n                                    lstm_layer_norm_epsilon,\n                                    lstm_dropout)\n        joiner = _Joiner(output_dim, num_symbols, activation = joiner_activation)\n        super().__init__(transcriber,\n                         predictor,\n                         joiner)\n    \n    def forward(self,\n                src: torch.Tensor,\n                src_lengths: torch.Tensor,\n                tgt: torch.Tensor,\n                tgt_lengths: torch.Tensor,\n                predictor_state: Optional[List[torch.Tensor]] = None\n                ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n        \"\"\"\n        Returns:\n            (torch.Tensor, torch.Tensor, torch.Tensor, List[List[torch.Tensor]])\n                torch.Tensor\n                    joint network output, with shape\n                    `(B, max output src length, max out tgt length, out dim)`\n        \"\"\"\n        src_encodings, src_lengths = self.transcriber(\n            input = src,\n            lengths = src_lengths\n        )\n        tgt_encodings, tgt_lengths, predictor_state = self.predictor(\n            input = tgt,\n            lengths = tgt_lengths,\n            state = predictor_state\n        )\n        output, src_lengths, tgt_lengths = self.joiner(\n            source_encodings = src_encodings,\n            source_lengths = src_lengths,\n            target_encodings = tgt_encodings,\n            target_lengths = tgt_lengths\n        )\n\n        return (output,\n                src_lengths,\n                tgt_lengths,\n                predictor_state)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:36:06.054962Z","iopub.execute_input":"2024-07-23T12:36:06.055403Z","iopub.status.idle":"2024-07-23T12:36:06.068971Z","shell.execute_reply.started":"2024-07-23T12:36:06.055371Z","shell.execute_reply":"2024-07-23T12:36:06.067731Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class ConformerTransducer(RNNT):\n    def __init__(self,\n                 input_dim: int,\n                 output_dim: int,\n                 time_reduction_stride: int,\n                 conformer_input_dim: int,\n                 conformer_ffn_dim: int,\n                 conformer_num_layers: int,\n                 conformer_num_heads: int,\n                 conformer_depthwise_conv_kernel_size: int,\n                 conformer_dropout: float,\n                 num_symbols: int,\n                 symbol_embedding_dim: int,\n                 num_lstm_layers: int,\n                 lstm_hidden_dim: int,\n                 lstm_layer_norm: bool = False,\n                 lstm_layer_norm_epsilon: float = 1e-5,\n                 lstm_dropout: float = 0.0,\n                 joiner_activation: str = 'relu') -> None:\n        transcriber = _ConformerEncoder(input_dim,\n                                             output_dim,\n                                             time_reduction_stride,\n                                             conformer_input_dim,\n                                             conformer_ffn_dim,\n                                             conformer_num_layers,\n                                             conformer_num_heads,\n                                             conformer_depthwise_conv_kernel_size,\n                                             conformer_dropout).to('cuda:0')\n        predictor = _Predictor(num_symbols,\n                                    output_dim,\n                                    symbol_embedding_dim,\n                                    num_lstm_layers,\n                                    lstm_hidden_dim,\n                                    lstm_layer_norm,\n                                    lstm_layer_norm_epsilon,\n                                    lstm_dropout).to('cuda:0')\n        joiner = _Joiner(output_dim, num_symbols, activation = joiner_activation).to('cuda:1')\n        super().__init__(transcriber,\n                         predictor,\n                         joiner)\n    \n    def forward(self,\n                src: torch.Tensor,\n                src_lengths: torch.Tensor,\n                tgt: torch.Tensor,\n                tgt_lengths: torch.Tensor,\n                predictor_state: Optional[List[torch.Tensor]] = None\n                ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n        \"\"\"\n        Returns:\n            (torch.Tensor, torch.Tensor, torch.Tensor, List[List[torch.Tensor]])\n                torch.Tensor\n                    joint network output, with shape\n                    `(B, max output src length, max out tgt length, out dim)`\n        \"\"\"\n        src_encodings, src_lengths = self.transcriber(\n            input = src.to('cuda:0'),\n            lengths = src_lengths.to('cuda:0')\n        )\n        tgt_encodings, tgt_lengths, predictor_state = self.predictor(\n            input = tgt.to('cuda:0'),\n            lengths = tgt_lengths.to('cuda:0'),\n            state = [x.to('cuda:0') for x in predictor_state] \\\n                           if predictor_state is not None else None\n        )\n        output, src_lengths, tgt_lengths = self.joiner(\n            source_encodings = src_encodings.to('cuda:1'),\n            source_lengths = src_lengths.to('cuda:1'),\n            target_encodings = tgt_encodings.to('cuda:1'),\n            target_lengths = tgt_lengths.to('cuda:1')\n        )\n\n        return (output,\n                src_lengths,\n                tgt_lengths,\n                predictor_state)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:30:38.798130Z","iopub.execute_input":"2024-07-23T12:30:38.798556Z","iopub.status.idle":"2024-07-23T12:30:38.815078Z","shell.execute_reply.started":"2024-07-23T12:30:38.798519Z","shell.execute_reply":"2024-07-23T12:30:38.814089Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"@add_to_class(ConformerTransducer)\ndef generate(self, \n             input: torch.Tensor, \n             input_lengths: torch.Tensor,\n             max_tgt_lengths: torch.Tensor = 400):\n    y_batch = []\n    B = len(input)\n    enc_out, enc_lengths = self.transcriber(input, input_lengths)\n    for b in range(B):\n        t = 0; u = 0;\n        y = [BOS_IDX]\n        predictor_state = None\n        while t < enc_lengths[b] and u < max_tgt_lengths:\n            predictor_in = torch.tensor([y[-1]], device = input.device).reshape(1, 1)\n            predictor_out, _, predictor_state = self.predictor(predictor_in, None, predictor_state)\n            transcriber_out = enc_out[b, t].reshape(1, 1, -1)\n            joiner_out, _, _ = self.joiner(transcriber_out,\n                                     None,\n                                     predictor_out,\n                                     None)\n            argmax = joiner_out.max(-1)[1].item()\n            if argmax == PAD_IDX:\n                t += 1\n            elif argmax == EOS_IDX:\n                break\n            else:\n                u += 1\n                y.append(argmax)\n        y_batch.append(y[1:])\n    return y_batch","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:36:23.483700Z","iopub.execute_input":"2024-07-23T12:36:23.484753Z","iopub.status.idle":"2024-07-23T12:36:23.493861Z","shell.execute_reply.started":"2024-07-23T12:36:23.484718Z","shell.execute_reply":"2024-07-23T12:36:23.492439Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Pipelining","metadata":{}},{"cell_type":"code","source":"class ConformerTransducer_training(S.SModule, ConformerTransducer):\n    def __init__(self, \n                 split_size: int,\n                 lr: Optional[Callable] = 0.0001,\n                 *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.lr = lr\n        self.split_size = split_size\n\n    def forward(self,\n                src: torch.Tensor,\n                src_lengths: torch.Tensor,\n                tgt: torch.Tensor,\n                tgt_lengths: torch.Tensor,\n                predictor_state: Optional[List[torch.Tensor]] = None\n                ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n        src_splits = iter(src.split(self.split_size, dim = 0))\n        src_len_splits = iter(src_lengths.split(self.split_size, dim = 0))\n        tgt_splits = iter(tgt.split(self.split_size, dim = 0))\n        tgt_len_splits = iter(tgt_lengths.split(self.split_size, dim = 0))\n        # predictor_state_splits = iter()\n\n        src_next = next(src_splits)\n        src_len_next = next(src_len_splits)\n        tgt_next = next(tgt_splits)\n        tgt_len_next = next(tgt_len_splits)\n        src_prev, src_len_prev = self.transcriber(input = src_next,\n                                                  lengths = src_len_next)\n        tgt_prev, tgt_len_prev, predictor_state_prev = self.predictor(input = tgt_next,\n                                                                      lengths = tgt_len_next,\n                                                                      state = predictor_state)\n        ret_output = []\n        ret_src_lens = []\n        ret_tgt_lens = []\n        for (src_next, src_len_next, tgt_next, tgt_len_next) in zip(src_splits, \n                                                                    src_len_splits,\n                                                                    tgt_splits,\n                                                                    tgt_len_splits):\n            out, src_lens, tgt_lens = self.joiner(src_prev,\n                                                  src_len_prev,\n                                                  tgt_prev,\n                                                  tgt_len_prev)\n            ret_output.append(out)\n            ret_src_lens.append(src_lens)\n            ret_tgt_lens.append(tgt_lens)\n            \n            src_prev, src_len_prev = self.transcriber(input = src_next,\n                                                  lengths = src_len_next)\n            tgt_prev, tgt_len_prev, predictor_state_prev = self.predictor(input = tgt_next,\n                                                                      lengths = tgt_len_next,\n                                                                      state = predictor_state)\n        out, src_lens, tgt_lens = self.joiner(src_prev,\n                                              src_len_prev,\n                                              tgt_prev,\n                                              tgt_len_prev)\n        ret_output.append(out)\n        ret_src_lens.append(src_lens)\n        ret_tgt_lens.append(tgt_lens)\n        return torch.cat(ret_output), torch.cat(ret_src_lens), torch.cat(ret_tgt_lens), predictor_state","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:36:24.167773Z","iopub.execute_input":"2024-07-23T12:36:24.168149Z","iopub.status.idle":"2024-07-23T12:36:24.181855Z","shell.execute_reply.started":"2024-07-23T12:36:24.168117Z","shell.execute_reply":"2024-07-23T12:36:24.180433Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class ConformerTransducer_training(S.SModule, ConformerTransducer):\n    def __init__(self, \n                 split_size: int,\n                 lr: Optional[Callable] = 0.0001,\n                 *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.lr = lr\n        self.split_size = split_size\n\n    def forward(self,\n                src: torch.Tensor,\n                src_lengths: torch.Tensor,\n                tgt: torch.Tensor,\n                tgt_lengths: torch.Tensor,\n                predictor_state: Optional[List[torch.Tensor]] = None\n                ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n        src_splits = iter(src.split(self.split_size, dim = 0))\n        src_len_splits = iter(src_lengths.split(self.split_size, dim = 0))\n        tgt_splits = iter(tgt.split(self.split_size, dim = 0))\n        tgt_len_splits = iter(tgt_lengths.split(self.split_size, dim = 0))\n        # predictor_state_splits = iter()\n\n        src_next = next(src_splits)\n        src_len_next = next(src_len_splits)\n        tgt_next = next(tgt_splits)\n        tgt_len_next = next(tgt_len_splits)\n        src_prev, src_len_prev = self.transcriber(input = src_next.to('cuda:0'),\n                                                  lengths = src_len_next.to('cuda:0'))\n        tgt_prev, tgt_len_prev, predictor_state_prev = self.predictor(input = tgt_next.to('cuda:0'),\n                                                                      lengths = tgt_len_next.to('cuda:0'),\n                                                                      state = predictor_state)\n        ret_output = []\n        ret_src_lens = []\n        ret_tgt_lens = []\n        for (src_next, src_len_next, tgt_next, tgt_len_next) in zip(src_splits, \n                                                                    src_len_splits,\n                                                                    tgt_splits,\n                                                                    tgt_len_splits):\n            out, src_lens, tgt_lens = self.joiner(src_prev.to('cuda:1'),\n                                                  src_len_prev.to('cuda:1'),\n                                                  tgt_prev.to('cuda:1'),\n                                                  tgt_len_prev.to('cuda:1'))\n            ret_output.append(out)\n            ret_src_lens.append(src_lens)\n            ret_tgt_lens.append(tgt_lens)\n            \n            src_prev, src_len_prev = self.transcriber(input = src_next.to('cuda:0'),\n                                                  lengths = src_len_next.to('cuda:0'))\n            tgt_prev, tgt_len_prev, predictor_state_prev = self.predictor(input = tgt_next.to('cuda:0'),\n                                                                      lengths = tgt_len_next.to('cuda:0'),\n                                                                      state = predictor_state)\n        out, src_lens, tgt_lens = self.joiner(src_prev.to('cuda:1'),\n                                              src_len_prev.to('cuda:1'),\n                                              tgt_prev.to('cuda:1'),\n                                              tgt_len_prev.to('cuda:1'))\n        ret_output.append(out)\n        ret_src_lens.append(src_lens)\n        ret_tgt_lens.append(tgt_lens)\n        return torch.cat(ret_output), torch.cat(ret_src_lens), torch.cat(ret_tgt_lens), predictor_state","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:07:53.774399Z","iopub.execute_input":"2024-07-19T19:07:53.775380Z","iopub.status.idle":"2024-07-19T19:07:53.792459Z","shell.execute_reply.started":"2024-07-19T19:07:53.775342Z","shell.execute_reply":"2024-07-19T19:07:53.791368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@add_to_class(ConformerTransducer_training)\ndef loss(self, logits, targets, logit_lengths, target_lengths):\n    return torchaudio.functional.rnnt_loss(logits,\n                                           targets,\n                                           logit_lengths,\n                                           target_lengths,\n                                           blank = PAD_IDX,\n                                           reduction = 'mean')\n\n@add_to_class(ConformerTransducer_training)\ndef training_step(self, batch, batch_idx):\n    src, tgt, src_lengths, tgt_lengths = batch\n    dec_input = tgt\n    dec_target = tgt[:, 1:]\n\n    preds, src_lengths, tgt_lengths, predictor_state = self.forward(src,\n                                                                    src_lengths = src_lengths,\n                                                                    tgt = dec_input,\n                                                                    tgt_lengths = tgt_lengths)\n    del dec_input\n    del src\n    del tgt\n    \n    loss = self.loss(preds, dec_target.contiguous().to('cuda:1'), src_lengths, tgt_lengths)\n    self.log(\"train_loss\", loss, pbar = True, train_logging = True)\n    return loss\n\n@add_to_class(ConformerTransducer_training)\ndef validation_step(self, batch, batch_idx):\n    src, tgt, src_lengths, tgt_lengths = batch\n    dec_input = tgt\n    dec_target = tgt[:, 1:]\n\n    preds, src_lengths, tgt_lengths, predictor_state = self.forward(src,\n                                                                    src_lengths = src_lengths,\n                                                                    tgt = dec_input,\n                                                                    tgt_lengths = tgt_lengths)\n    del dec_input\n    del src\n    del tgt\n    \n    loss = self.loss(preds, dec_target.contiguous().to('cuda:1'), src_lengths, tgt_lengths)\n    self.log(\"val_loss\", loss, pbar = True, train_logging = False)\n\n@add_to_class(ConformerTransducer_training)\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr = 0.0005,\n                                 weight_decay = 1e-6,\n                                 betas = (0.9, 0.98),\n                                 eps = 1e-9)\n    return optimizer\n\n@add_to_class(ConformerTransducer_training)\ndef optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure):\n    # update params\n    optimizer.step(closure = optimizer_closure)\n\n    # manually warm up lr without a scheduler\n    lr = self.lr.calculate_lr(epoch)\n\n    for pg in optimizer.param_groups:\n        pg['lr'] = lr\n    self.log('lr', lr, pbar = True, train_logging = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:07:55.951200Z","iopub.execute_input":"2024-07-19T19:07:55.951558Z","iopub.status.idle":"2024-07-19T19:07:55.964824Z","shell.execute_reply.started":"2024-07-19T19:07:55.951529Z","shell.execute_reply":"2024-07-19T19:07:55.963888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomLRScheduler:\n    def __init__(self,\n                 init_lr = 0.0005,\n                 lr_after_warmup = 0.001,\n                 final_lr = 0.0001,\n                 warmup_epochs = 5,\n                 decay_epochs = 100):\n        self.init_lr = init_lr\n        self.lr_after_warmup = lr_after_warmup\n        self.final_lr = final_lr\n        self.warmup_epochs = warmup_epochs\n        self.decay_epochs = decay_epochs\n\n    def calculate_lr(self, epoch):\n        \"\"\"\n        Linear warm up - linear decay\n        \"\"\"\n        warmup_lr = self.init_lr + ((self.lr_after_warmup - self.init_lr)/(self.warmup_epochs - 1))*epoch\n        decay_lr = max(self.final_lr,\n                       self.lr_after_warmup\n                       - (epoch - self.warmup_epochs)\n                       *(self.lr_after_warmup - self.final_lr)\n                       /self.decay_epochs)\n        return min(warmup_lr, decay_lr)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:07:58.040575Z","iopub.execute_input":"2024-07-19T19:07:58.041120Z","iopub.status.idle":"2024-07-19T19:07:58.050259Z","shell.execute_reply.started":"2024-07-19T19:07:58.041079Z","shell.execute_reply":"2024-07-19T19:07:58.048955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback = S.ModelCheckpoint(dirpath = '/kaggle/working',\n                                      save_top_k = 7, monitor = 'val_loss',\n                                      mode = 'min',\n                                      filename = 'conformer_rnnt-10m-bpe-ls100-epoch:%02d-val_loss:%.4f')","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:07:58.417852Z","iopub.execute_input":"2024-07-19T19:07:58.418786Z","iopub.status.idle":"2024-07-19T19:07:58.423840Z","shell.execute_reply.started":"2024-07-19T19:07:58.418745Z","shell.execute_reply":"2024-07-19T19:07:58.422748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = CustomLRScheduler()\nmodel = ConformerTransducer_training(input_dim = 80,\n                                     output_dim = 512,\n                                     time_reduction_stride = 4,\n                                     conformer_input_dim = 256,\n                                     conformer_ffn_dim = 512,\n                                     conformer_num_layers = 8,\n                                     conformer_num_heads = 4,\n                                     conformer_depthwise_conv_kernel_size = 31,\n                                     conformer_dropout = 0.1,\n                                     num_symbols = len(data.tokenizer.vocab),\n                                     symbol_embedding_dim = 128,\n                                     num_lstm_layers = 1,\n                                     lstm_hidden_dim = 320,\n                                     lstm_layer_norm = True,\n                                     lstm_layer_norm_epsilon = 1e-5,\n                                     lstm_dropout = 0.3,\n                                     joiner_activation = \"tanh\",\n                                     lr = lr,\n                                     split_size = 8)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:08:05.818199Z","iopub.execute_input":"2024-07-19T19:08:05.819337Z","iopub.status.idle":"2024-07-19T19:08:06.300502Z","shell.execute_reply.started":"2024-07-19T19:08:05.819290Z","shell.execute_reply":"2024-07-19T19:08:06.299520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(pytorch_total_params)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:08:09.746307Z","iopub.execute_input":"2024-07-19T19:08:09.747297Z","iopub.status.idle":"2024-07-19T19:08:09.757071Z","shell.execute_reply.started":"2024-07-19T19:08:09.747258Z","shell.execute_reply":"2024-07-19T19:08:09.755735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = S.Trainer(callbacks = [checkpoint_callback],\n                   enable_checkpointing = True,\n                   max_epochs = 55,\n                   gradient_clip_val = 1)\ntrainer.fit(model, data, ckpt_path = '/kaggle/input/ckpt-10m/conformer_rnnt-10m-bpe-ls100-epoch_49-val_loss_10.4149.ckpt')","metadata":{"execution":{"iopub.status.busy":"2024-07-19T19:08:12.323180Z","iopub.execute_input":"2024-07-19T19:08:12.323587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"model = ConformerTransducer_training(input_dim = 80,\n                                     output_dim = 512,\n                                     time_reduction_stride = 4,\n                                     conformer_input_dim = 256,\n                                     conformer_ffn_dim = 512,\n                                     conformer_num_layers = 8,\n                                     conformer_num_heads = 4,\n                                     conformer_depthwise_conv_kernel_size = 31,\n                                     conformer_dropout = 0.1,\n                                     num_symbols = len(data.tokenizer.vocab),\n                                     symbol_embedding_dim = 128,\n                                     num_lstm_layers = 1,\n                                     lstm_hidden_dim = 320,\n                                     lstm_layer_norm = True,\n                                     lstm_layer_norm_epsilon = 1e-5,\n                                     lstm_dropout = 0.3,\n                                     joiner_activation = \"tanh\",\n                                     lr = 0.001,\n                                     split_size = 8)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:36:29.105271Z","iopub.execute_input":"2024-07-23T12:36:29.105795Z","iopub.status.idle":"2024-07-23T12:36:29.254229Z","shell.execute_reply.started":"2024-07-23T12:36:29.105763Z","shell.execute_reply":"2024-07-23T12:36:29.252948Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"ckpt_path = '/kaggle/input/ckpt-10m/conformer_rnnt-10m-bpe-ls100-epoch_54-val_loss_9.8642.ckpt'\nckpt = torch.load(ckpt_path, map_location = device)\nmodel.load_state_dict(ckpt['state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:37:08.277624Z","iopub.execute_input":"2024-07-23T12:37:08.278015Z","iopub.status.idle":"2024-07-23T12:37:09.115029Z","shell.execute_reply.started":"2024-07-23T12:37:08.277987Z","shell.execute_reply":"2024-07-23T12:37:09.113627Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"def transcribe(model: Callable,\n               tokenizer: Callable,\n               src: torch.Tensor,\n               src_lengths: torch.Tensor,\n               max_tgt_lengths: int):\n    model.eval()\n    texts = []\n    token_outs = model.generate(src, src_lengths, max_tgt_lengths)\n    for token_out in token_outs:\n        texts.append(tokenizer.itos(token_out))\n    return texts","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:38:37.719099Z","iopub.execute_input":"2024-07-23T12:38:37.719510Z","iopub.status.idle":"2024-07-23T12:38:37.725832Z","shell.execute_reply.started":"2024-07-23T12:38:37.719475Z","shell.execute_reply":"2024-07-23T12:38:37.724397Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"test_dataset = LibriSpeech100('/kaggle/input/librispeech-clean',\n                              subset = 'test-clean')\nspec0, text0 = test_dataset[0]\nspec0 = spec0.transpose(1, 2).contiguous()\nprint(spec0.shape)\nprint(text0)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:38:48.990982Z","iopub.execute_input":"2024-07-23T12:38:48.991375Z","iopub.status.idle":"2024-07-23T12:38:49.276158Z","shell.execute_reply.started":"2024-07-23T12:38:48.991345Z","shell.execute_reply":"2024-07-23T12:38:49.274933Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"torch.Size([1, 1044, 80])\nHE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE\n","output_type":"stream"}]},{"cell_type":"code","source":"transcribe(model, data.tokenizer, spec0, torch.tensor([spec0.shape[1]]), 400)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:39:18.218093Z","iopub.execute_input":"2024-07-23T12:39:18.218489Z","iopub.status.idle":"2024-07-23T12:39:19.029007Z","shell.execute_reply.started":"2024-07-23T12:39:18.218460Z","shell.execute_reply":"2024-07-23T12:39:19.027663Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"['heled there wouldy stewor diner turnips and carots and broves potes and fat mutend pieces to belade out in thic peper flower fatine sas']"},"metadata":{}}]},{"cell_type":"code","source":"spec0, text0 = test_dataset[1]\nspec0 = spec0.transpose(1, 2).contiguous()\nprint(spec0.shape)\nprint(text0)\ntranscribe(model, data.tokenizer, spec0, torch.tensor([spec0.shape[1]]), 400)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:39:52.390167Z","iopub.execute_input":"2024-07-23T12:39:52.390579Z","iopub.status.idle":"2024-07-23T12:39:52.603703Z","shell.execute_reply.started":"2024-07-23T12:39:52.390533Z","shell.execute_reply":"2024-07-23T12:39:52.602707Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"torch.Size([1, 328, 80])\nSTUFF IT INTO YOU HIS BELLY COUNSELLED HIM\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"['stufid inte his belly counchold him']"},"metadata":{}}]},{"cell_type":"code","source":"spec0, text0 = test_dataset[2]\nspec0 = spec0.transpose(1, 2).contiguous()\nprint(spec0.shape)\nprint(text0)\ntranscribe(model, data.tokenizer, spec0, torch.tensor([spec0.shape[1]]), 400)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:41:27.939071Z","iopub.execute_input":"2024-07-23T12:41:27.939494Z","iopub.status.idle":"2024-07-23T12:41:28.353657Z","shell.execute_reply.started":"2024-07-23T12:41:27.939463Z","shell.execute_reply":"2024-07-23T12:41:28.352490Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"torch.Size([1, 663, 80])\nAFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"['after early nightfall the yellow lams would light up hear and there the squlade qughter of the brofles']"},"metadata":{}}]},{"cell_type":"code","source":"spec0, text0 = test_dataset[3]\nspec0 = spec0.transpose(1, 2).contiguous()\nprint(spec0.shape)\nprint(text0)\ntranscribe(model, data.tokenizer, spec0, torch.tensor([spec0.shape[1]]), 400)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:42:03.340160Z","iopub.execute_input":"2024-07-23T12:42:03.340609Z","iopub.status.idle":"2024-07-23T12:42:03.493182Z","shell.execute_reply.started":"2024-07-23T12:42:03.340551Z","shell.execute_reply":"2024-07-23T12:42:03.491960Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"torch.Size([1, 269, 80])\nHELLO BERTIE ANY GOOD IN YOUR MIND\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"['our berte anik goooden your mine']"},"metadata":{}}]},{"cell_type":"code","source":"spec0, text0 = test_dataset[5]\nspec0 = spec0.transpose(1, 2).contiguous()\nprint(spec0.shape)\nprint(text0)\ntranscribe(model, data.tokenizer, spec0, torch.tensor([spec0.shape[1]]), 400)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:42:32.083161Z","iopub.execute_input":"2024-07-23T12:42:32.083539Z","iopub.status.idle":"2024-07-23T12:42:32.730015Z","shell.execute_reply.started":"2024-07-23T12:42:32.083512Z","shell.execute_reply":"2024-07-23T12:42:32.728702Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"torch.Size([1, 964, 80])\nTHE MUSIC CAME NEARER AND HE RECALLED THE WORDS THE WORDS OF SHELLEY'S FRAGMENT UPON THE MOON WANDERING COMPANIONLESS PALE FOR WEARINESS\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"['the mus it came nerer and he recauled a words the words of shellis frikemet upon the moon wandering compance pale for wearines']"},"metadata":{}}]},{"cell_type":"code","source":"spec0, text0 = data.train_dataset[4]\nspec0 = spec0.transpose(1, 2).contiguous()\nprint(spec0.shape)\nprint(text0)\ntranscribe(model, data.tokenizer, spec0, torch.tensor([spec0.shape[1]]), 400)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T12:43:42.546103Z","iopub.execute_input":"2024-07-23T12:43:42.546529Z","iopub.status.idle":"2024-07-23T12:43:43.366582Z","shell.execute_reply.started":"2024-07-23T12:43:42.546495Z","shell.execute_reply":"2024-07-23T12:43:43.365459Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"torch.Size([1, 1252, 80])\nBUT MISSUS RACHEL LYNDE WAS ONE OF THOSE CAPABLE CREATURES WHO CAN MANAGE THEIR OWN CONCERNS AND THOSE OF OTHER FOLKS INTO THE BARGAIN SHE WAS A NOTABLE HOUSEWIFE HER WORK WAS ALWAYS DONE AND WELL DONE SHE RAN THE SEWING CIRCLE\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"['but mis rachieal lyne was one of thos cable creatures whoken manacitar owne concerance and thoes of other foke int the barken she was anotable hous wife her wor was always don and welled them she ran the soing cirple']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}